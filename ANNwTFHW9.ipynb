{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANNwTFHW9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnUoo-NzYmr_"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sascha-senko/TensorflowCourse/blob/HSinger04/ANNwTFHW9.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXk1CAw1_JgQ"
      },
      "source": [
        "## Global TODO: Only for Hermann\n",
        "\n",
        "* Add the 2 digits as inputs to each datum\n",
        "* Add labels to each datum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj9MbknNYnHD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b7660b8-5aae-47c7-fd18-29a07d671250"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import random\n",
        "%load_ext tensorboard\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Conv2DTranspose, \\\n",
        " Reshape, MaxPooling2D, Dropout, BatchNormalization, UpSampling2D, ReLU, \\\n",
        " ELU, Layer\n",
        "from tensorflow import debugging as debug\n",
        "import tensorflow_probability as tfp\n",
        "from functools import partial"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-ejCaubiq7V"
      },
      "source": [
        "## Define some constants for dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpxDyHAlisNe"
      },
      "source": [
        "# arbitrarily set. Feel free to change these\n",
        "DATA_SIZE = 10000\n",
        "SEQ_SIZE = 25\n",
        "SHUFFLE_SIZE = DATA_SIZE\n",
        "PREFETCH_SIZE = tf.data.experimental.AUTOTUNE\n",
        "BATCH_SIZE = 32"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr_NH9xSYrYp"
      },
      "source": [
        "## Define Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIXpWjYkYr0l"
      },
      "source": [
        "# helper function\n",
        "my_one_hot = lambda x: tf.one_hot(tf.cast(x, tf.int32), depth=10, axis=-1)\n",
        "\n",
        "def digit_sequence(data_size, size=SEQ_SIZE):\n",
        "    num = 0\n",
        "    label = None\n",
        "\n",
        "    while num < data_size:\n",
        "        # get sequence\n",
        "        seq = np.random.randint(10, size=size)   \n",
        "\n",
        "        # get context\n",
        "\n",
        "        # get all unique digits of sequence\n",
        "        digits = np.unique(seq)\n",
        "        # context digits\n",
        "        context = np.random.choice(digits, size=2, replace=False)\n",
        "\n",
        "        # get label\n",
        "\n",
        "        # counts how much more often the first context digit was observed over the second\n",
        "        first_vs_second_occurance = 0\n",
        "\n",
        "        for i in range(size):\n",
        "            digit = seq[i]\n",
        "            if digit == context[0]:\n",
        "                first_vs_second_occurance += 1\n",
        "            elif digit == context[1]:\n",
        "                first_vs_second_occurance -= 1\n",
        "\n",
        "        if first_vs_second_occurance >= 0:        \n",
        "            label = 0\n",
        "        else:\n",
        "            label = 1    \n",
        "\n",
        "        # one-hotify seq and context\n",
        "        seq = my_one_hot(seq)\n",
        "        context = my_one_hot(context)    \n",
        "\n",
        "        # yield the two context digits, the sequence and the label\n",
        "        yield seq, context, label \n",
        "        num += 1\n",
        "\n",
        "x_train = tf.data.Dataset.from_generator(digit_sequence, args=[DATA_SIZE], output_signature=(\n",
        "    tf.TensorSpec((25, 10)), \n",
        "    tf.TensorSpec((2, 10)),\n",
        "    tf.TensorSpec(())\n",
        ")).batch(BATCH_SIZE)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbJF5wJnadUn"
      },
      "source": [
        "## LSTM Cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJD2-yoO-PSq"
      },
      "source": [
        "class LSTM_Cell(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(LSTM_Cell, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        #gates\n",
        "        self.input_gate = Dense(hidden_size, activation=\"sigmoid\")\n",
        "        # setting forget bias to one initially is important, \n",
        "        # probably because the very first hidden and cell state that gets fed in \n",
        "        # call is just a dummy zero vector and doesn't provide any information  \n",
        "        self.forget_gate = Dense(hidden_size, bias_initializer='ones', activation=\"sigmoid\")\n",
        "        self.output_gate = Dense(hidden_size, activation=\"sigmoid\")\n",
        "        self.cell_state_candidates = Dense(hidden_size, activation=\"tanh\")\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, input, hidden_state, cell_state):\n",
        "        # x is 1-D\n",
        "        concat_input = tf.concat([hidden_state, input], axis=-1) \n",
        "        new_cell_state = cell_state * self.forget_gate(concat_input) \n",
        "        new_cell_state += self.input_gate(concat_input) * self.cell_state_candidates(concat_input)\n",
        "        new_hidden_state = tf.keras.activations.tanh(cell_state) * self.output_gate(concat_input) # new hidden state is also output\n",
        "        return new_cell_state, new_hidden_state      "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1iifUJxUJ0-"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7NNTNvvUKcc"
      },
      "source": [
        "class LSTM(Model):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        # TODO: read in layer\n",
        "        self.hidden_size = hidden_size\n",
        "        self.LSTM_Cell = LSTM_Cell(hidden_size)\n",
        "        # < 0.5 for first context digit occuring more often, >= 0.5 for second\n",
        "        self.read_out = Dense(1, activation='sigmoid')\n",
        "        \n",
        "    # TODO: Let's see if call works, as I am initializing a state  \n",
        "    # if I end up using tf.Variable, make sure it's untrainable\n",
        "    #@tf.function\n",
        "    def call(self, x):\n",
        "        results = tf.TensorArray(tf.float32, size=SEQ_SIZE)\n",
        "\n",
        "        hidden_state = tf.zeros((BATCH_SIZE, self.hidden_size))\n",
        "        cell_state = tf.zeros((BATCH_SIZE, self.hidden_size))\n",
        "        \n",
        "        # TODO: check if it's zero everytime right here with debug\n",
        "        for index in range(SEQ_SIZE):\n",
        "            digit = x[:,index,:]\n",
        "            # TODO: check if compatible with tf.function\n",
        "            cell_state, hidden_state = self.LSTM_Cell(digit, hidden_state, cell_state)\n",
        "            results.write(index, hidden_state)\n",
        "\n",
        "        # TODO\n",
        "        results = tf.transpose(results.stack(), perm=[1,0,2])\n",
        "\n",
        "        output = self.read_out(results)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6T40tU_Zhl9"
      },
      "source": [
        "## Define some constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNO5r8R9ZjU9"
      },
      "source": [
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 0.0001   \n",
        "OPTIMIZER = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
        "BCE = tf.keras.losses.BinaryCrossentropy() \n",
        "NUM_BATCHES = (int(x_train.cardinality()))\n",
        "HIDDEN_SIZE = 10\n",
        "\n",
        "import datetime\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "lstm_log_dir = 'logs/gradient_tape/' + current_time + '/lstm'\n",
        "train_writer = tf.summary.create_file_writer(lstm_log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdDHxiqkhg1Q"
      },
      "source": [
        "@tf.function\n",
        "def train_step(model, inputs, label, optimizer):\n",
        "    # loss_object and optimizer_object are instances of respective tensorflow classes\n",
        "    with tf.GradientTape() as tape:\n",
        "        prediction = model(inputs)[:,-1,0]\n",
        "        loss = BCE(label, prediction)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    # update weights  \n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    prediction = tf.cast(tf.math.round(prediction), label.dtype)\n",
        "    correct = tf.math.equal(prediction, correct)\n",
        "\n",
        "    return loss, correct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVydiHTYnvTo"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ok2Ni4Ensst"
      },
      "source": [
        "# TODO: Comment about BPTT\n",
        "# TODO: Regression or classification problem?\n",
        "# TODO: args\n",
        "# TODO: weight update still incorrect. Remember that we have an output for each part\n",
        "@tf.function\n",
        "def train_step(model, inputs, label, optimizer):\n",
        "    # loss_object and optimizer_object are instances of respective tensorflow classes\n",
        "    with tf.GradientTape() as tape:\n",
        "        # we are only interested in the overall prediction\n",
        "        prediction = model(inputs)[:,-1,0]\n",
        "        loss = BCE(label, prediction)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    # update weights  \n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    prediction = tf.cast(tf.math.round(prediction), label.dtype)\n",
        "    correct = tf.math.equal(prediction, correct)\n",
        "\n",
        "    return loss, correct\n",
        "\n",
        "# TODO: epoch needs to be tf.function compatible\n",
        "#@tf.function\n",
        "def one_epoch(model, optimizer, loss_tracker, accuracy_tracker, train_data, epoch):\n",
        "    # reset statistics\n",
        "    loss_tracker.reset_states()\n",
        "    accuracy_tracker.reset_states()\n",
        "\n",
        "    # TODO: not just input, but also other things\n",
        "    for inputs, contexts, labels in train_data:\n",
        "        \n",
        "        # TODO: dunno if recursive tf.function worked so well\n",
        "        loss, accuracy = train_step(model, inputs, labels, optimizer)\n",
        "\n",
        "        loss_tracker.update_state(loss)\n",
        "        accuracy_tracker.update_state(accuracy)\n",
        "\n",
        "    # Write statistics into summary\n",
        "    with train_writer.as_default():\n",
        "        tf.summary.scalar('loss', loss_tracker.result(), step=epoch)\n",
        "        tf.summary.scalar('accuracy', accuracy_tracker.result(), step=epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uESQeQfwtGVy"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9LywnzRZm-f"
      },
      "source": [
        "# Clear any logs from previous runs\n",
        "%rm -rf ./logs/\n",
        "\n",
        "# remove all active models for memory purposes\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = LSTM(HIDDEN_SIZE)\n",
        "\n",
        "loss_tracker = tf.keras.metrics.Mean()\n",
        "accuracy_tracker = tf.keras.metrics.Mean()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print('Epoch: ' + str(epoch+1))\n",
        "    one_epoch(model, OPTIMIZER, loss_tracker, accuracy_tracker, x_train, epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5X7Z1RnpH1G"
      },
      "source": [
        "# Open tensorboard\n",
        "%tensorboard --logdir logs/gradient_tape"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}