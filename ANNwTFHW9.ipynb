{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANNwTFHW9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnUoo-NzYmr_"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sascha-senko/TensorflowCourse/blob/main/ANNwTFHW9.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXk1CAw1_JgQ"
      },
      "source": [
        "## Global TODO: Only for Hermann\n",
        "\n",
        "* Add the 2 digits as inputs to each datum\n",
        "* Add labels to each datum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj9MbknNYnHD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfbcab75-3421-4c09-fbf1-be495793b108"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import random\n",
        "%load_ext tensorboard\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Conv2DTranspose, \\\n",
        " Reshape, MaxPooling2D, Dropout, BatchNormalization, UpSampling2D, ReLU, \\\n",
        " ELU, Layer\n",
        "from tensorflow import debugging as debug\n",
        "import tensorflow_probability as tfp\n",
        "from functools import partial"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-ejCaubiq7V"
      },
      "source": [
        "## Define some constants for dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpxDyHAlisNe"
      },
      "source": [
        "# arbitrarily set. Feel free to change these\n",
        "DATA_SIZE = 10000\n",
        "SEQ_SIZE = 25\n",
        "SHUFFLE_SIZE = DATA_SIZE\n",
        "PREFETCH_SIZE = tf.data.experimental.AUTOTUNE\n",
        "BATCH_SIZE = 32"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr_NH9xSYrYp"
      },
      "source": [
        "## Define Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIXpWjYkYr0l"
      },
      "source": [
        "# helper function\n",
        "my_one_hot = lambda x: tf.one_hot(tf.cast(x, tf.int32), depth=10, axis=-1)\n",
        "\n",
        "def digit_sequence(data_size, size=SEQ_SIZE):\n",
        "    num = 0\n",
        "    label = None\n",
        "\n",
        "    while num < data_size:\n",
        "        # get sequence\n",
        "        seq = np.random.randint(10, size=size)   \n",
        "\n",
        "        # get context\n",
        "\n",
        "        # get all unique digits of sequence\n",
        "        digits = np.unique(seq)\n",
        "        # context digits\n",
        "        context = np.random.choice(digits, size=2, replace=False)\n",
        "\n",
        "        # get label\n",
        "\n",
        "        # counts how much more often the first context digit was observed over the second\n",
        "        first_vs_second_occurance = 0\n",
        "\n",
        "        for i in range(size):\n",
        "            digit = seq[i]\n",
        "            if digit == context[0]:\n",
        "                first_vs_second_occurance += 1\n",
        "            elif digit == context[1]:\n",
        "                first_vs_second_occurance -= 1\n",
        "\n",
        "        if first_vs_second_occurance >= 0:        \n",
        "            label = 0\n",
        "        else:\n",
        "            label = 1    \n",
        "\n",
        "        # one-hotify seq and context\n",
        "        seq = my_one_hot(seq)\n",
        "        context = my_one_hot(context)    \n",
        "\n",
        "        # yield the two context digits, the sequence and the label\n",
        "        yield seq, context, label \n",
        "        num += 1\n",
        "\n",
        "x_train = tf.data.Dataset.from_generator(digit_sequence, args=[DATA_SIZE], output_signature=(\n",
        "    tf.TensorSpec((25, 10)), \n",
        "    tf.TensorSpec((2, 10)),\n",
        "    tf.TensorSpec(())\n",
        ")).batch(BATCH_SIZE)"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbJF5wJnadUn"
      },
      "source": [
        "## LSTM Cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJD2-yoO-PSq"
      },
      "source": [
        "class LSTM_Cell(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(LSTM_Cell, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        #gates\n",
        "        self.input_gate = Dense(hidden_size, activation=\"sigmoid\")\n",
        "        # setting forget bias to one initially is important, \n",
        "        # probably because the very first hidden and cell state that gets fed in \n",
        "        # call is just a dummy zero vector and doesn't provide any information  \n",
        "        self.forget_gate = Dense(hidden_size, bias_initializer='ones', activation=\"sigmoid\")\n",
        "        self.output_gate = Dense(hidden_size, activation=\"sigmoid\")\n",
        "        self.cell_state_candidates = Dense(hidden_size, activation=\"tanh\")\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, input, hidden_state, cell_state):\n",
        "        # x is 1-D\n",
        "        concat_input = tf.concat([hidden_state, input], axis=-1) \n",
        "        new_cell_state = cell_state * self.forget_gate(concat_input) \n",
        "        new_cell_state += self.input_gate(concat_input) * self.cell_state_candidates(concat_input)\n",
        "        new_hidden_state = tf.keras.activations.tanh(cell_state) * self.output_gate(concat_input) # new hidden state is also output\n",
        "        return new_cell_state, new_hidden_state      "
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1iifUJxUJ0-"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7NNTNvvUKcc"
      },
      "source": [
        "class LSTM(Model):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        # plain old linear function. Each dimension represents\n",
        "        # the corresponding digit of the sequence with information \n",
        "        # about the context digits mixed in\n",
        "        self.read_in = Dense(SEQ_SIZE)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.LSTM_Cell = LSTM_Cell(hidden_size)\n",
        "        # < 0.5 for first context digit occuring more often, >= 0.5 for second\n",
        "        self.read_out = Dense(1, activation='sigmoid')\n",
        "        \n",
        "    # TODO: Let's see if call works, as I am initializing a state  \n",
        "    # if I end up using tf.Variable, make sure it's untrainable\n",
        "    #@tf.function\n",
        "    def call(self, x):\n",
        "        results = tf.TensorArray(tf.float32, size=SEQ_SIZE)\n",
        "        encoded_seq = self.read_in(tf.concat([x, context], 1))\n",
        "\n",
        "        hidden_state = tf.zeros((BATCH_SIZE, self.hidden_size))\n",
        "        cell_state = tf.zeros((BATCH_SIZE, self.hidden_size))\n",
        "        \n",
        "        # TODO: check if it's zero everytime right here with debug\n",
        "        for index in range(SEQ_SIZE):\n",
        "            digit = x[:,index,:]\n",
        "            # TODO: check if compatible with tf.function\n",
        "            cell_state, hidden_state = self.LSTM_Cell(digit, hidden_state, cell_state)\n",
        "            results.write(index, hidden_state)\n",
        "\n",
        "        # TODO\n",
        "        output = tf.transpose(results.stack(), perm=[1,0,2])\n",
        "        # close results\n",
        "        results.close()\n",
        "\n",
        "        output = self.read_out(output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6T40tU_Zhl9"
      },
      "source": [
        "## Define some constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNO5r8R9ZjU9"
      },
      "source": [
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 0.0001   \n",
        "OPTIMIZER = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
        "BCE = tf.keras.losses.BinaryCrossentropy() \n",
        "NUM_BATCHES = (int(x_train.cardinality()))\n",
        "HIDDEN_SIZE = 10\n",
        "\n",
        "import datetime\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "lstm_log_dir = 'logs/gradient_tape/' + current_time + '/lstm'\n",
        "train_writer = tf.summary.create_file_writer(lstm_log_dir)"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVydiHTYnvTo"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ok2Ni4Ensst"
      },
      "source": [
        "# TODO: Comment about BPTT\n",
        "# TODO: Regression or classification problem?\n",
        "# TODO: args\n",
        "# TODO: weight update still incorrect. Remember that we have an output for each part\n",
        "@tf.function\n",
        "def train_step(model, inputs, contexts, label, optimizer):\n",
        "    # loss_object and optimizer_object are instances of respective tensorflow classes\n",
        "    with tf.GradientTape() as tape:\n",
        "        # we are only interested in the overall prediction\n",
        "        prediction = model(inputs, contexts)[:,-1,0]\n",
        "        loss = BCE(label, prediction)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    # update weights  \n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    prediction = tf.cast(tf.math.round(prediction), label.dtype)\n",
        "    correct = tf.math.equal(prediction, correct)\n",
        "\n",
        "    return loss, correct\n",
        "\n",
        "# TODO: epoch needs to be tf.function compatible\n",
        "#@tf.function\n",
        "def one_epoch(model, optimizer, loss_tracker, accuracy_tracker, train_data, epoch):\n",
        "    # reset statistics\n",
        "    loss_tracker.reset_states()\n",
        "    accuracy_tracker.reset_states()\n",
        "\n",
        "    # TODO: not just input, but also other things\n",
        "    for inputs, contexts, labels in train_data:\n",
        "        \n",
        "        # TODO: dunno if recursive tf.function worked so well\n",
        "        loss, accuracy = train_step(model, inputs, contexts, labels, optimizer)\n",
        "\n",
        "        loss_tracker.update_state(loss)\n",
        "        accuracy_tracker.update_state(accuracy)\n",
        "\n",
        "    # Write statistics into summary\n",
        "    with train_writer.as_default():\n",
        "        tf.summary.scalar('loss', loss_tracker.result(), step=epoch)\n",
        "        tf.summary.scalar('accuracy', accuracy_tracker.result(), step=epoch)"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uESQeQfwtGVy"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9LywnzRZm-f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "09ce055a-61f4-443b-ce99-172a9d7990fa"
      },
      "source": [
        "# Clear any logs from previous runs\n",
        "%rm -rf ./logs/\n",
        "\n",
        "# remove all active models for memory purposes\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = LSTM(HIDDEN_SIZE)\n",
        "\n",
        "loss_tracker = tf.keras.metrics.Mean()\n",
        "accuracy_tracker = tf.keras.metrics.Mean()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print('Epoch: ' + str(epoch+1))\n",
        "    one_epoch(model, OPTIMIZER, loss_tracker, accuracy_tracker, x_train, epoch)"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-187-bc0dfe4a403c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mone_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_tracker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_tracker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-186-9c2516a86e33>\u001b[0m in \u001b[0;36mone_epoch\u001b[0;34m(model, optimizer, loss_tracker, accuracy_tracker, train_data, epoch)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# TODO: dunno if recursive tf.function worked so well\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mloss_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    <ipython-input-129-9c2516a86e33>:10 train_step  *\n        prediction = model(inputs, contexts)[:,-1,0]\n\n    TypeError: tf__call() takes 2 positional arguments but 3 were given\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5X7Z1RnpH1G"
      },
      "source": [
        "# Open tensorboard\n",
        "%tensorboard --logdir logs/gradient_tape"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}