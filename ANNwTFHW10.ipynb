{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANNwTFHW9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnUoo-NzYmr_"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sascha-senko/TensorflowCourse/blob/main/ANNwTFHW10.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf5-glbon-07"
      },
      "source": [
        "# General stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj9MbknNYnHD"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import random\n",
        "%load_ext tensorboard\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Conv2DTranspose, \\\n",
        " Reshape, MaxPooling2D, Dropout, BatchNormalization, UpSampling2D, ReLU, \\\n",
        " ELU, Layer, Embedding\n",
        "from tensorflow import debugging as debug\n",
        "import tensorflow_probability as tfp\n",
        "from functools import partial\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "import re\n",
        "from collections import Counter\n",
        "from scipy.spatial.distance import cosine"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpxDyHAlisNe"
      },
      "source": [
        "# Clear any logs from previous runs\n",
        "%rm -rf ./logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEw3CwLDTFqz"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j84VSHVtTGiG"
      },
      "source": [
        "def data_pipeline(data):\n",
        "    \"\"\" helper function for data pipeline - does all the things we need \"\"\"\n",
        "\n",
        "    data = data.shuffle(buffer_size=SHUFFLE_SIZE)\n",
        "    data = data.batch(BATCH_SIZE)\n",
        "    data = data.prefetch(PREFETCH_SIZE)\n",
        "    if DATA_SIZE:\n",
        "        data = data.take(DATA_SIZE)\n",
        "    return data\n",
        "\n",
        "@tf.function\n",
        "def train(model, optimizer, loss_tracker, accuracy_tracker, train_data, num_epochs, train_func):\n",
        "    for epoch in tf.range(num_epochs):\n",
        "        tf.print('Epoch: ' + str(epoch+1))\n",
        "        # reset statistics\n",
        "        loss_tracker.reset_states()\n",
        "        accuracy_tracker.reset_states()\n",
        "\n",
        "        for inputs, labels in train_data:\n",
        "            \n",
        "            loss, accuracy = train_func(model, inputs, labels, optimizer, epoch)\n",
        "\n",
        "            loss_tracker.update_state(loss)\n",
        "            accuracy_tracker.update_state(accuracy)\n",
        "\n",
        "        # Write statistics into summary\n",
        "        with train_writer.as_default():\n",
        "            tf.summary.scalar('loss', loss_tracker.result(), step=tf.cast(epoch, tf.int64))\n",
        "            tf.summary.scalar('accuracy', accuracy_tracker.result(), step=tf.cast(epoch, tf.int64))    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1Gb14QmAwAE"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APmBa9L3AxF8"
      },
      "source": [
        "whole_text = tfds.load('tiny_shakespeare', split='train')\n",
        "for total_text in whole_text:\n",
        "    whole_text = str(total_text['text'].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqueCEKXAZ1A"
      },
      "source": [
        "# SkipGram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYyZYYahYDxy"
      },
      "source": [
        "## Some constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muI0biJfYDTz"
      },
      "source": [
        "CONTEXT_WINDOW_HALF = 2\n",
        "# we only allow CONTEXT_WINDOW to be an even number\n",
        "CONTEXT_WINDOW = 2 * CONTEXT_WINDOW_HALF\n",
        "SUB_SAMPLE = 0.001\n",
        "EMBED_SIZE = 64\n",
        "VOCAB_SIZE = 10000\n",
        "NUM_SAMPLED = 20\n",
        "SHUFFLE_SIZE = 40000\n",
        "# set to 0 if we are not debugging\n",
        "DATA_SIZE = 0\n",
        "PREFETCH_SIZE = tf.data.experimental.AUTOTUNE\n",
        "BATCH_SIZE = 30\n",
        "NO_SPECIAL = False\n",
        "UNKNOWN_WORD = \"UNKNOWN\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWciiniE3DYw"
      },
      "source": [
        "## Deal with words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy1V91473Evx"
      },
      "source": [
        "# lower case everything\n",
        "x_train = whole_text.lower()\n",
        "# Get rid of all new lines\n",
        "# TODO: make sure that this works correctly. Observed some pretty weird behavior\n",
        "x_train = x_train.split(r\"\\\\n\")\n",
        "words = []\n",
        "# tokenize\n",
        "for line in x_train:\n",
        "    temp = []\n",
        "    tokens = nltk.word_tokenize(line)\n",
        "    # if True, get rid of all special characters\n",
        "    if NO_SPECIAL:\n",
        "        for token in tokens:\n",
        "            if re.search('\\w+', token):\n",
        "                temp.append(token)\n",
        "    else:\n",
        "        temp = tokens            \n",
        "    words.extend(temp)\n",
        "\n",
        "x_train = words\n",
        "words = Counter(words).most_common(VOCAB_SIZE)\n",
        "id_to_word = [word for word, _ in words]\n",
        "# garbage collect this\n",
        "del words\n",
        "\n",
        "word_to_id = {}\n",
        "for id, word in enumerate(id_to_word):\n",
        "    word_to_id[word] = id\n",
        "\n",
        "# unknown words get id len(id_to_word)\n",
        "id_to_word.append(UNKNOWN_WORD)\n",
        "\n",
        "# apply word_to_id map on x_train\n",
        "x_train = [word_to_id.get(x, VOCAB_SIZE) for x in x_train]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXla4f5zYmxw"
      },
      "source": [
        "## Finish up dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yATc1j_UYpXy"
      },
      "source": [
        "y_train = []\n",
        "# Get all targets\n",
        "for i, input in enumerate(x_train[CONTEXT_WINDOW_HALF:-CONTEXT_WINDOW_HALF]):\n",
        "    # we leave out the first and last CONTEXT_WINDOW_HALF words out\n",
        "    i = i + CONTEXT_WINDOW_HALF\n",
        "    # append context words as labels\n",
        "    for j in range(CONTEXT_WINDOW_HALF):\n",
        "        y_train.append(x_train[i-1-j])\n",
        "        y_train.append(x_train[i+1+j])\n",
        "\n",
        "assert len(y_train) == (len(x_train) - CONTEXT_WINDOW) * CONTEXT_WINDOW, \\\n",
        "\"is: \" + str(len(input_target_pairs)) + \" must: \" + str((len(x_train) - CONTEXT_WINDOW) * CONTEXT_WINDOW)\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((np.repeat(x_train[CONTEXT_WINDOW_HALF:-CONTEXT_WINDOW_HALF], CONTEXT_WINDOW), y_train))\n",
        "\n",
        "train_data = data_pipeline(train_data)"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKXFMtUhf-2l"
      },
      "source": [
        "# # Check if correct - check only works if we leave out data_pipeline\n",
        "# for i, (input, target) in enumerate(train_data):\n",
        "#     print(input)\n",
        "#     print(target)\n",
        "#     if i == 3:\n",
        "#         break\n",
        "\n",
        "# print(x_train[:5])        "
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1iifUJxUJ0-"
      },
      "source": [
        "## Define SkipGram "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7NNTNvvUKcc"
      },
      "source": [
        "class SkipGram(Layer):\n",
        "    def __init__(self, vocab_size, embed_size, num_sampled):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.embed = Embedding(vocab_size, embed_size)\n",
        "        self.score_matrix = tf.Variable(tf.random.normal([vocab_size, embed_size]))\n",
        "        self.score_biases = tf.Variable(tf.zeros([vocab_size]))\n",
        "        self.num_sampled = num_sampled\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, context, x):\n",
        "        x_embed = self.embed(x)\n",
        "        context = tf.expand_dims(context, -1)\n",
        "        loss = tf.nn.nce_loss(self.score_matrix, self.score_biases, context, \n",
        "                              x_embed, self.num_sampled, self.vocab_size)\n",
        "        return loss"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA8atR--OGOQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "106ee156-f825-4ac4-8a7a-e25924e7152e"
      },
      "source": [
        "model = SkipGram(VOCAB_SIZE+1, 20, 4)\n",
        "for input, label in train_data:\n",
        "    model(label, input)\n",
        "    break\n",
        "print(model.embed.trainable_variables[0].dtype) "
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function SkipGram.call at 0x7fb278aa4598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function SkipGram.call at 0x7fb278aa4598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<dtype: 'float32'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6T40tU_Zhl9"
      },
      "source": [
        "## Define some constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNO5r8R9ZjU9"
      },
      "source": [
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 10\n",
        "OPTIMIZER = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
        "EMBED_TRACKER = tf.TensorArray(tf.float32, size=NUM_EPOCHS, dynamic_size=False)\n",
        "\n",
        "import datetime\n",
        "\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "skip_gram_log_dir = 'logs/gradient_tape/' + current_time + '/skip_gram'\n",
        "train_writer = tf.summary.create_file_writer(skip_gram_log_dir)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXIkpJF-_iWS"
      },
      "source": [
        "## SkipGram train step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXrmafgv_kY6"
      },
      "source": [
        "@tf.function\n",
        "def skip_gram_train_step(model, inputs, labels, optimizer, epoch):\n",
        "    global EMBED_TRACKER\n",
        "    # loss_object and optimizer_object are instances of respective tensorflow classes\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = model(inputs, label)\n",
        "        # average over the batch manually\n",
        "        loss = tf.math.reduce_mean(loss)\n",
        "        debug.assert_shapes([(loss, ())])\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    # update weights  \n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    # record embedding\n",
        "    # TODO: use EMBED_TRACKER to track embedding matrix\n",
        "    embed_matrix = model.embed.trainable_variables[0]\n",
        "    # We want a frozen version of the embedding matrix\n",
        "    embed_matrix = tf.identity(embed_matrix)\n",
        "    EMBED_TRACKER = EMBED_TRACKER.write(0, embed_matrix)\n",
        "\n",
        "    # dummy value for accuracy\n",
        "    return loss, 0.0"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uESQeQfwtGVy"
      },
      "source": [
        "## Train SkipGram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9LywnzRZm-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b9bc0e7-5488-4d4a-9757-b6d7cc5d6392"
      },
      "source": [
        "# remove all active models for memory purposes\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = SkipGram(VOCAB_SIZE+1, EMBED_SIZE, NUM_SAMPLED)\n",
        "\n",
        "loss_tracker = tf.keras.metrics.Mean()\n",
        "accuracy_tracker = tf.keras.metrics.Mean()\n",
        "\n",
        "train(model, OPTIMIZER, loss_tracker, accuracy_tracker, train_data, NUM_EPOCHS, skip_gram_train_step)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: Tensor(\"while/add:0\", shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-4CHJEjBkYv"
      },
      "source": [
        "## Compute cosine distances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD3rcH0VBtkf"
      },
      "source": [
        "# TODO: Possibly, the dimensions still need to be switched. Compare with\n",
        "# tf.transpose(results.stack(), perm=[1,0,2]) from HW9\n",
        "EMBED_MATRICES = EMBED_TRACKER.stack()\n",
        "\n",
        "COMPARISON_WORDS_STR = [\"queen\", \"throne\", \"wine\"]\n",
        "# TODO: get embedding vector of comparison words as well\n",
        "COMPARISON_WORDS = np.array([word_to_id[word] for word in COMPARISON_WORDS_STR])\n",
        "\n",
        "num_words = len(COMPARISON_WORDS_STR)    \n",
        "    \n",
        "cosine_dist = np.empty((num_words, NUM_EPOCHS, VOCAB_SIZE+1))\n",
        "\n",
        "for word_ind, word in enumerate(COMPARISON_WORDS):\n",
        "    for matrix_ind in range(NUM_EPOCHS):\n",
        "        comparison_vec = EMBED_MATRICES[matrix_ind, word, :]\n",
        "        for vector_ind in range(VOCAB_SIZE+1):\n",
        "            temp = cosine(EMBED_MATRICES[matrix_ind, vector_ind, :], comparison_vec)  \n",
        "            cosine_dist[word_ind, matrix_ind, vector_ind] = temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFKplc_sr-ef"
      },
      "source": [
        "## Print out k nearest neighbors for each epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkaRewTYsEWO"
      },
      "source": [
        "cosine_sorted = np.empty_like(cosine_dist, np.int32)\n",
        "\n",
        "for word_ind in range(num_words):\n",
        "    for matrix_ind in range(NUM_EPOCHS):\n",
        "        cosine_sorted[word_ind, matrix_ind, :] = np.argsort(cosine_dist[word_ind, matrix_ind, :])\n",
        "\n",
        "k = 10\n",
        "\n",
        "for matrix_ind in range(NUM_EPOCHS):\n",
        "    print(\"Epoch: \" + str(j))\n",
        "    print(2 * \"\\n\")\n",
        "    for word_ind, word in enumerate(COMPARISON_WORDS_STR):\n",
        "        print(word + \"'s distances:\")\n",
        "        print(\"\\n\")\n",
        "        # We leave out the first word since that's just the word itself\n",
        "        for i in range(1, k+1):\n",
        "            # TODO: remove\n",
        "            word_value = cosine_sorted[word_ind, matrix_ind, i]\n",
        "            dist = cosine_dist[word_ind, matrix_ind, word_value]\n",
        "            # TODO: also print actual word string\n",
        "            print(str(l) + \". word: \" + str(dist))    \n",
        "        print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQOZfxf4Acqm"
      },
      "source": [
        "# Shakespeare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzu7y9DpVG04"
      },
      "source": [
        "## Some constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TtayH0pVJZl"
      },
      "source": [
        "SEQ_LENGTH = 20\n",
        "SHUFFLE_SIZE = 40000\n",
        "PREFETCH_SIZE = tf.data.experimental.AUTOTUNE\n",
        "BATCH_SIZE = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIOZHQhiAzjy"
      },
      "source": [
        "## Preprocess Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n53i9cb0A7Q5"
      },
      "source": [
        "# lower case everything\n",
        "x_train = whole_text.lower()\n",
        "x_train = x_train\n",
        "# Replace all new lines with white space\n",
        "x_train = re.sub(r\"\\\\n\", \" \", x_train) \n",
        "\n",
        "id_to_word = list(set(x_train))\n",
        "\n",
        "word_to_id = {}\n",
        "for i, word in enumerate(id_to_word):\n",
        "    word_to_id[word] = i\n",
        "\n",
        "# Map the characters to numbers\n",
        "x_train = [word_to_id[x] for x in x_train]\n",
        "\n",
        "# we will need this for target\n",
        "last_num = x_train[-1]\n",
        "\n",
        "# Create input subsequences\n",
        "x_train = [x_train[i:i+SEQ_LENGTH] for i in range(len(x_train)-SEQ_LENGTH-1)]\n",
        "\n",
        "# Generate labels\n",
        "y_train = [x_train[i][-1] for i in range(1, len(x_train))] \n",
        "\n",
        "# add label for last subsequence\n",
        "y_train.append(last_num)\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "\n",
        "train_data = data_pipeline(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgHWBDwROBDr"
      },
      "source": [
        "## Text Generation Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYjq49z_OCaC"
      },
      "source": [
        "class Shakespeare_RNN(Model):\n",
        "  def __init__(self, vocab_size, embed_size=10, num_cells=4, hidden_size=256, return_sequences=False):\n",
        "    super(Shakespeare_RNN, self).__init__()\n",
        "\n",
        "    hidden_state_dim = [BATCH_SIZE] \n",
        "    hidden_state_dim.extend([hidden_size for _ in range(num_cells)])\n",
        "    self.initial_state = tf.zeros(hidden_state_dim)\n",
        "    \n",
        "    self.embedding = Embedding(vocab_size, embed_size)\n",
        "    self.rnn = tf.keras.layers.RNN([tf.keras.layers.SimpleRNNCell(hidden_size) for _ in range(num_cells)], return_sequences)\n",
        "    self.readout_layer = tf.keras.layers.Dense(units=vocab_size, activation='softmax')\n",
        "\n",
        "  @tf.function  \n",
        "  def call(self, x, training):\n",
        "    x = self.embedding(x)\n",
        "    x = self.rnn(x, initial_state=self.initial_state)\n",
        "    x = self.readout_layer(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t0gpGKRgQp2"
      },
      "source": [
        "## Define some constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXvA3fBjgR1m"
      },
      "source": [
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 10\n",
        "OPTIMIZER = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
        "\n",
        "import datetime\n",
        "\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "rnn_log_dir = 'logs/gradient_tape/' + current_time + '/rnn'\n",
        "train_writer = tf.summary.create_file_writer(rnn_log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zPekb7_c8nV"
      },
      "source": [
        "## RNN train step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrT5uNmidCop"
      },
      "source": [
        "LOSS = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "# TODO\n",
        "#@tf.function\n",
        "def rnn_train_step(model, inputs, labels, optimizer, epoch):\n",
        "    # loss_object and optimizer_object are instances of respective tensorflow classes\n",
        "    with tf.GradientTape() as tape:\n",
        "        pred = model(inputs, tf.constant(False))\n",
        "        # debug.assert_shapes([\n",
        "        #                      (pred, (BATCH_SIZE, len(word_to_id))),\n",
        "        #                      (labels, (BATCH_SIZE,))\n",
        "        # ])\n",
        "        loss = LOSS(labels, pred)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    # update weights  \n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    # dummy value for accuracy\n",
        "    return loss, 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72NX-shLfeed"
      },
      "source": [
        "model = Shakespeare_RNN(len(word_to_id), num_cells=1)\n",
        "\n",
        "loss_tracker = tf.keras.metrics.Mean()\n",
        "accuracy_tracker = tf.keras.metrics.Mean()\n",
        "\n",
        "train(model, OPTIMIZER, loss_tracker, accuracy_tracker, train_data, NUM_EPOCHS, rnn_train_step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mTQJuSTA7vo"
      },
      "source": [
        "# Run TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5X7Z1RnpH1G"
      },
      "source": [
        "# Open tensorboard\n",
        "%tensorboard --logdir logs/gradient_tape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n5sMrKf6bDi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}