{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANNwTFHW9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnUoo-NzYmr_"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sascha-senko/TensorflowCourse/blob/main/ANNwTFHW10.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf5-glbon-07"
      },
      "source": [
        "# General stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj9MbknNYnHD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "code",
        "outputId": "eac6527f-5f8b-4821-8fe0-85923c0a7844"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import random\n",
        "%load_ext tensorboard\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Conv2DTranspose, \\\n",
        " Reshape, MaxPooling2D, Dropout, BatchNormalization, UpSampling2D, ReLU, \\\n",
        " ELU, Layer, Embedding\n",
        "from tensorflow import debugging as debug\n",
        "import tensorflow_probability as tfp\n",
        "from functools import partial\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "import re\n",
        "from collections import Counter\n",
        "from scipy.spatial.distance import cosine"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpxDyHAlisNe",
        "cellView": "code"
      },
      "source": [
        "# Clear any logs from previous runs\n",
        "%rm -rf ./logs/\n",
        "\n",
        "# if true, decorate specific functions with @tf.function. \n",
        "TF_FUNCTION = True\n",
        "EMBED_MATRICES = []"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEw3CwLDTFqz"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j84VSHVtTGiG",
        "cellView": "code"
      },
      "source": [
        "def data_pipeline(data):\n",
        "    \"\"\" helper function for data pipeline - does all the things we need \"\"\"\n",
        "\n",
        "    data = data.shuffle(buffer_size=SHUFFLE_SIZE)\n",
        "    data = data.batch(BATCH_SIZE)\n",
        "    data = data.prefetch(PREFETCH_SIZE)\n",
        "    if DATA_SIZE:\n",
        "        data = data.take(DATA_SIZE)\n",
        "    return data\n",
        "\n",
        "def train(model_name, model, optimizer, loss_tracker, train_data, num_epochs, train_func):\n",
        "    for epoch in range(num_epochs):\n",
        "        print(epoch)\n",
        "        # reset statistics\n",
        "        loss_tracker.reset_states()\n",
        "\n",
        "        embed_matrix = train_func(model, train_data, optimizer, loss_tracker)\n",
        "\n",
        "        # Write statistics into summary\n",
        "        with train_writer.as_default():\n",
        "            tf.summary.scalar('loss', loss_tracker.result(), step=epoch)    \n",
        "\n",
        "        if \"skip\" in model_name.lower():\n",
        "            EMBED_MATRICES.append(embed_matrix)\n",
        "        elif \"rnn\" in model_name.lower():\n",
        "            pass\n",
        "        else:\n",
        "            raise RuntimeError            "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1Gb14QmAwAE"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APmBa9L3AxF8",
        "cellView": "code"
      },
      "source": [
        "whole_text = tfds.load('tiny_shakespeare', split='train')\n",
        "for total_text in whole_text:\n",
        "    whole_text = str(total_text['text'].numpy())"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqueCEKXAZ1A"
      },
      "source": [
        "# SkipGram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYyZYYahYDxy"
      },
      "source": [
        "## Some constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muI0biJfYDTz",
        "cellView": "code"
      },
      "source": [
        "CONTEXT_WINDOW_HALF = 2\n",
        "# we only allow CONTEXT_WINDOW to be an even number\n",
        "CONTEXT_WINDOW = 2 * CONTEXT_WINDOW_HALF\n",
        "SUB_SAMPLE = 0.001\n",
        "EMBED_SIZE = 64\n",
        "VOCAB_SIZE = 10000\n",
        "NUM_SAMPLED = 20\n",
        "SHUFFLE_SIZE = 40000\n",
        "PREFETCH_SIZE = tf.data.experimental.AUTOTUNE\n",
        "BATCH_SIZE = 30\n",
        "NO_SPECIAL = True\n",
        "# set as such to avoid a bug\n",
        "DATA_SIZE = 0\n",
        "if NO_SPECIAL:\n",
        "    DATA_SIZE = 23260\n",
        "else:    \n",
        "    DATA_SIZE = 0\n",
        "unknown_word = \"UNKNOWN\""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWciiniE3DYw"
      },
      "source": [
        "## Deal with words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy1V91473Evx",
        "cellView": "code"
      },
      "source": [
        "# lower case everything\n",
        "x_train = whole_text.lower()\n",
        "# Get rid of all new lines\n",
        "x_train = x_train.split(r\"\\n\")\n",
        "words = []\n",
        "# tokenize\n",
        "for line in x_train:\n",
        "    temp = []\n",
        "    tokens = nltk.word_tokenize(line)\n",
        "    # if True, get rid of all special characters\n",
        "    if NO_SPECIAL:\n",
        "        for token in tokens:\n",
        "            if re.search('\\w+', token):\n",
        "                temp.append(token)\n",
        "    else:\n",
        "        temp = tokens            \n",
        "    words.extend(temp)\n",
        "\n",
        "x_train = words\n",
        "words = Counter(words).most_common(VOCAB_SIZE)\n",
        "id_to_word = [word for word, _ in words]\n",
        "# garbage collect this\n",
        "del words\n",
        "\n",
        "word_to_id = {}\n",
        "for id, word in enumerate(id_to_word):\n",
        "    word_to_id[word] = id\n",
        "\n",
        "# unknown words get id len(id_to_word)\n",
        "id_to_word.append(unknown_word)\n",
        "\n",
        "# apply word_to_id map on x_train\n",
        "x_train = [word_to_id.get(x, VOCAB_SIZE) for x in x_train]"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXla4f5zYmxw"
      },
      "source": [
        "## Finish up dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yATc1j_UYpXy",
        "cellView": "code"
      },
      "source": [
        "y_train = []\n",
        "# Get all targets\n",
        "for i, inputs in enumerate(x_train[CONTEXT_WINDOW_HALF:-CONTEXT_WINDOW_HALF]):\n",
        "    # we leave out the first and last CONTEXT_WINDOW_HALF words out\n",
        "    i = i + CONTEXT_WINDOW_HALF\n",
        "    # append context words as labels\n",
        "    for j in range(CONTEXT_WINDOW_HALF):\n",
        "        y_train.append(x_train[i-1-j])\n",
        "        y_train.append(x_train[i+1+j])\n",
        "\n",
        "assert len(y_train) == (len(x_train) - CONTEXT_WINDOW) * CONTEXT_WINDOW, \\\n",
        "\"is: \" + str(len(input_target_pairs)) + \" must: \" + str((len(x_train) - CONTEXT_WINDOW) * CONTEXT_WINDOW)\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((np.repeat(x_train[CONTEXT_WINDOW_HALF:-CONTEXT_WINDOW_HALF], CONTEXT_WINDOW), y_train))\n",
        "\n",
        "train_data = data_pipeline(train_data)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKXFMtUhf-2l",
        "cellView": "code"
      },
      "source": [
        "# # Check if correct - check only works if we leave out data_pipeline\n",
        "# for i, (input, target) in enumerate(train_data):\n",
        "#     print(input)\n",
        "#     print(target)\n",
        "#     if i == 3:\n",
        "#         break\n",
        "\n",
        "# print(x_train[:5])        "
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1iifUJxUJ0-"
      },
      "source": [
        "## Define SkipGram "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7NNTNvvUKcc",
        "cellView": "code"
      },
      "source": [
        "class SkipGram(Layer):\n",
        "    def __init__(self, vocab_size, embed_size, num_sampled):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.embed = Embedding(vocab_size, embed_size)\n",
        "        self.score_matrix = tf.Variable(tf.random.normal([vocab_size, embed_size]))\n",
        "        self.score_biases = tf.Variable(tf.zeros([vocab_size]))\n",
        "        self.num_sampled = num_sampled\n",
        "        self.vocab_size = vocab_size\n",
        "        if TF_FUNCTION:\n",
        "            self.call = tf.function(self.call)\n",
        "\n",
        "    def call(self, context, x):\n",
        "        x_embed = self.embed(x)\n",
        "        context = tf.expand_dims(context, -1)\n",
        "        loss = tf.nn.nce_loss(self.score_matrix, self.score_biases, context, \n",
        "                                  x_embed, self.num_sampled, self.vocab_size) \n",
        "        return loss"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA8atR--OGOQ",
        "cellView": "code"
      },
      "source": [
        "# model = SkipGram(VOCAB_SIZE+1, 20, 4)\n",
        "# for input, label in train_data:\n",
        "#     model(label, input)\n",
        "#     break\n",
        "# print(model.embed.trainable_variables[0].dtype) "
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6T40tU_Zhl9"
      },
      "source": [
        "## Define some constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNO5r8R9ZjU9",
        "cellView": "code"
      },
      "source": [
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "OPTIMIZER = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
        "\n",
        "import datetime\n",
        "\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "skip_gram_log_dir = 'logs/gradient_tape/' + current_time + '/skip_gram'\n",
        "train_writer = tf.summary.create_file_writer(skip_gram_log_dir)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXIkpJF-_iWS"
      },
      "source": [
        "## SkipGram train step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXrmafgv_kY6",
        "cellView": "code"
      },
      "source": [
        "def skip_gram_train_step(model, train_data, optimizer, loss_tracker):\n",
        "\n",
        "    for inputs, labels in train_data:\n",
        "\n",
        "        # loss_object and optimizer_object are instances of respective tensorflow classes\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = model(inputs, labels)\n",
        "            # average over the batch manually\n",
        "            loss = tf.math.reduce_mean(loss)\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "        # update weights  \n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        loss_tracker.update_state(loss)\n",
        "\n",
        "    embed_matrix = model.embed.trainable_variables[0]\n",
        "\n",
        "    return embed_matrix\n",
        "\n",
        "if TF_FUNCTION:\n",
        "    skip_gram_train_step = tf.function(skip_gram_train_step)    "
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uESQeQfwtGVy"
      },
      "source": [
        "## Train SkipGram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9LywnzRZm-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "code",
        "outputId": "7c88e2cb-e25d-4be7-f1df-28939d9aad1a"
      },
      "source": [
        "# remove all active models for memory purposes\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = SkipGram(VOCAB_SIZE+1, EMBED_SIZE, NUM_SAMPLED)\n",
        "\n",
        "loss_tracker = tf.keras.metrics.Mean()\n",
        "\n",
        "train(\"SkipGram\", model, OPTIMIZER, loss_tracker, train_data, NUM_EPOCHS, skip_gram_train_step)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-4CHJEjBkYv"
      },
      "source": [
        "## Compute cosine distances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD3rcH0VBtkf",
        "cellView": "code"
      },
      "source": [
        "EMBED_MATRICES = np.array(EMBED_MATRICES)\n",
        "COMPARISON_WORDS_STR = [\"queen\", \"throne\", \"wine\"]\n",
        "COMPARISON_WORDS = np.array([word_to_id[word] for word in COMPARISON_WORDS_STR])\n",
        "\n",
        "num_words = len(COMPARISON_WORDS_STR)    \n",
        "    \n",
        "cosine_dist = np.empty((num_words, NUM_EPOCHS, VOCAB_SIZE+1))\n",
        "\n",
        "for word_ind, word in enumerate(COMPARISON_WORDS):\n",
        "    for matrix_ind in range(NUM_EPOCHS):\n",
        "        comparison_vec = EMBED_MATRICES[matrix_ind, word, :]\n",
        "        for vector_ind in range(VOCAB_SIZE+1):\n",
        "            # calculate cosine distance\n",
        "            temp = cosine(EMBED_MATRICES[matrix_ind, vector_ind, :], comparison_vec)  \n",
        "            cosine_dist[word_ind, matrix_ind, vector_ind] = temp\n",
        "\n",
        "cosine_sorted = np.empty_like(cosine_dist, np.int32)\n",
        "\n",
        "for word_ind in range(num_words):\n",
        "    for matrix_ind in range(NUM_EPOCHS):\n",
        "        cosine_sorted[word_ind, matrix_ind, :] = np.argsort(cosine_dist[word_ind, matrix_ind, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFKplc_sr-ef"
      },
      "source": [
        "## Print out k nearest neighbors for each epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkaRewTYsEWO",
        "cellView": "code"
      },
      "source": [
        "k = 10\n",
        "\n",
        "for matrix_ind in range(NUM_EPOCHS):\n",
        "    print(\"Epoch: \" + str(matrix_ind))\n",
        "    print(2 * \"\\n\")\n",
        "    for word_ind, word in enumerate(COMPARISON_WORDS_STR):\n",
        "        print(word + \"'s \" + str(k) + \" closest neighbors and distances:\")\n",
        "        print(\"\\n\")\n",
        "        # We leave out the closest neighbor since that's just the word itself\n",
        "        for i in range(1, k+1):\n",
        "            word_value = cosine_sorted[word_ind, matrix_ind, i]\n",
        "            dist = cosine_dist[word_ind, matrix_ind, word_value]\n",
        "            print(str(i) + \". word: \" + id_to_word[word_value] + \"; distance: \" + str(dist))    \n",
        "        print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQOZfxf4Acqm"
      },
      "source": [
        "# Shakespeare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzu7y9DpVG04"
      },
      "source": [
        "## Some constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TtayH0pVJZl",
        "cellView": "code"
      },
      "source": [
        "SEQ_LENGTH = 20\n",
        "SHUFFLE_SIZE = 40000\n",
        "PREFETCH_SIZE = tf.data.experimental.AUTOTUNE\n",
        "BATCH_SIZE = 30\n",
        "DATA_SIZE = 0\n",
        "unknown_word = \"$\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIOZHQhiAzjy"
      },
      "source": [
        "## Preprocess Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n53i9cb0A7Q5",
        "cellView": "code"
      },
      "source": [
        "# lower case everything\n",
        "x_train = whole_text.lower()\n",
        "x_train = x_train\n",
        "# Replace all new lines with white space\n",
        "x_train = re.sub(r\"\\\\n\", \" \", x_train) \n",
        "\n",
        "id_to_word_set = set(x_train)\n",
        "id_to_word = []\n",
        "# remove non-sensical characters, since we somehow have characters like \"&\"\n",
        "for word in id_to_word_set:\n",
        "    if re.search(\"\\w\", word) or word == \" \" or word == \".\":\n",
        "        id_to_word.append(word)\n",
        "\n",
        "word_to_id = {}\n",
        "for i, word in enumerate(id_to_word):\n",
        "    word_to_id[word] = i\n",
        "\n",
        "# Map the characters to numbers\n",
        "x_train = [word_to_id.get(x, len(word_to_id)) for x in x_train]\n",
        "\n",
        "# append unknown\n",
        "id_to_word.append(unknown_word)\n",
        "\n",
        "# we will need this for target later\n",
        "last_num = x_train[-1]\n",
        "\n",
        "# Create input subsequences\n",
        "x_train = [x_train[i:i+SEQ_LENGTH] for i in range(len(x_train)-SEQ_LENGTH-1)]\n",
        "\n",
        "# Generate labels\n",
        "y_train = [x_train[i][-1] for i in range(1, len(x_train))] \n",
        "\n",
        "# add label for last subsequence\n",
        "y_train.append(last_num)\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "\n",
        "train_data = data_pipeline(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgHWBDwROBDr"
      },
      "source": [
        "## Text Generation Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYjq49z_OCaC",
        "cellView": "code"
      },
      "source": [
        "class Shakespeare_RNN(Model):\n",
        "  def __init__(self, vocab_size, embed_size=10, num_cells=4, hidden_size=256, return_sequences=False):\n",
        "    super(Shakespeare_RNN, self).__init__()\n",
        "    \n",
        "    self.embedding = Embedding(vocab_size, embed_size)\n",
        "    self.rnn = tf.keras.layers.RNN([tf.keras.layers.SimpleRNNCell(hidden_size) for _ in range(num_cells)], return_sequences=return_sequences)\n",
        "    self.readout_layer = tf.keras.layers.Dense(units=vocab_size, activation='softmax')\n",
        "\n",
        "    if TF_FUNCTION:\n",
        "        self.call = tf.function(self.call)\n",
        "\n",
        "  def call(self, x, training):\n",
        "    x = self.embedding(x)\n",
        "    x = self.rnn(x)\n",
        "    x = self.readout_layer(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t0gpGKRgQp2"
      },
      "source": [
        "## Define some constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXvA3fBjgR1m",
        "cellView": "code"
      },
      "source": [
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "OPTIMIZER = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
        "\n",
        "import datetime\n",
        "\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "rnn_log_dir = 'logs/gradient_tape/' + current_time + '/rnn'\n",
        "train_writer = tf.summary.create_file_writer(rnn_log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zPekb7_c8nV"
      },
      "source": [
        "## RNN train step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrT5uNmidCop",
        "cellView": "code"
      },
      "source": [
        "LOSS = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "def rnn_train_step(model, train_data, optimizer, loss_tracker):\n",
        "    for inputs, labels in train_data:\n",
        "        # loss_object and optimizer_object are instances of respective tensorflow classes\n",
        "        with tf.GradientTape() as tape:\n",
        "            pred = model(inputs, tf.constant(False))\n",
        "            loss = LOSS(labels, pred)\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "        # update weights  \n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        loss_tracker.update_state(loss)\n",
        "\n",
        "if TF_FUNCTION:\n",
        "    rnn_train_step = tf.function(rnn_train_step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72NX-shLfeed",
        "cellView": "code"
      },
      "source": [
        "model = Shakespeare_RNN(len(id_to_word), num_cells=1)\n",
        "\n",
        "loss_tracker = tf.keras.metrics.Mean()\n",
        "\n",
        "train(\"RNN\", model, OPTIMIZER, loss_tracker, train_data, NUM_EPOCHS, rnn_train_step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj_I1X_bBhzi"
      },
      "source": [
        "## Text generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snSAMtqeBj8P",
        "cellView": "code"
      },
      "source": [
        "def generate_text(phrase, generated_length, model):\n",
        "    \n",
        "    out_text = phrase\n",
        "\n",
        "    phrase = phrase.lower()\n",
        "    phrase = [word_to_id[charac] for charac in phrase]\n",
        "\n",
        "    for _ in range(generated_length):\n",
        "        inp = tf.constant(phrase)\n",
        "        # batch\n",
        "        inp = tf.reshape(inp, (1, -1))\n",
        "        out = model(inp)\n",
        "        # sample character\n",
        "        out = tf.random.categorical(out, 1).numpy()[0][0]\n",
        "        # transform to word\n",
        "        phrase = phrase[1:]\n",
        "        phrase.append(out)\n",
        "        out = id_to_word[out]\n",
        "        out_text += out    \n",
        "\n",
        "    print(out_text)    \n",
        "\n",
        "phrase = \"\"\n",
        "while not len(phrase) == SEQ_LENGTH: \n",
        "    # example input: \"Thou shalt forgive m\"\n",
        "    phrase = input(\"Choose your phrase of length \" + str(SEQ_LENGTH) + \": \")\n",
        "\n",
        "generated_length = 100\n",
        "generate_text(phrase, generated_length, model)        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mTQJuSTA7vo"
      },
      "source": [
        "# Run TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5X7Z1RnpH1G",
        "cellView": "code"
      },
      "source": [
        "# Open tensorboard\n",
        "%tensorboard --logdir logs/gradient_tape"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}