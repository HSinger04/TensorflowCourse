{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "ANNwTFHW5.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "Tk3lP_umiEcA"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sascha-senko/TensorflowCourse/blob/main/ANNwTFHW7.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "If6lyQRtwScq"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SS5Zdc67wVjk"
   },
   "source": [
    "# TODO: Clean up imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, \\\n",
    "    BatchNormalization, ELU, Flatten, Dense, ZeroPadding2D, AveragePooling2D, \\\n",
    "    Layer, GlobalAveragePooling2D, concatenate\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from functools import partial  \n",
    "from tensorflow import debugging as debug"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNfHI-9Swfrk"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cowuKHOqwfAg"
   },
   "source": [
    "# TODO: load appropriate dataset. See what needs to be loaded first, though\n",
    "# Test set is necessary - take 1000 examples. Also, labels of test set, but not of training set, are necessary\n",
    "x = tfds.load(\n",
    "    'fashion_mnist', split=['train[:1%]', 'test[:1%]'])\n",
    "\n",
    "# TODO: remove\n",
    "print(\"bla\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKx2urxbwma1"
   },
   "source": [
    "## Inspect data set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "1vVMp_SLwopk",
    "outputId": "b677b56a-f2b8-4fe6-fe74-41ed75627206"
   },
   "source": [
    "fig, ax = plt.subplots(1,10)\n",
    "shapes = []\n",
    "\n",
    "\n",
    "# TODO: remove this\n",
    "num_to_name = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\",\n",
    "\"ship\", \"truck\"]\n",
    "\n",
    "for i, (img, lbl) in enumerate(train_data):\n",
    "    if i == 10:\n",
    "        break\n",
    "    ax[i].imshow(img, cmap='gray')\n",
    "    # TODO: remove this\n",
    "    ax[i].set_title(num_to_name[lbl.numpy()])\n",
    "    ax[i].axis(\"off\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMaxBk-2ws_6"
   },
   "source": [
    "## Define some constants"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5BeTRDjGwtpy"
   },
   "source": [
    "# arbitrarily set. Feel free to change these\n",
    "SHUFFLE_SIZE = train_data.cardinality()\n",
    "PREFETCH_SIZE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 128"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhOipCR-wxPZ"
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dMWeKpJqwx_Q"
   },
   "source": [
    "# TODO: only process input\n",
    "def process_inp_and_label(input, label, training):\n",
    "    \"\"\" rescale inputs and onehotify labels \"\"\"\n",
    "    new_input = input / 255\n",
    "    return new_input, tf.one_hot(label, 10)\n",
    "\n",
    "# TODO: @Hermann: See what stuff I need to look up online / externally first, note them down and look them up\n",
    "# See how I could load fashion mnist\n",
    "# Do I need to process training and test set, or just one?\n",
    "# TODO: do I actually have to process training and test dataset, or just one?\n",
    "# TODO: if just one: Remove the argument\n",
    "# TODO: Follow https://github.com/Maxoz99/ANNs-TensorFlow/blob/main/Homework05/homework05.ipynb for data pipeline\n",
    "def data_pipeline(data, training):\n",
    "    \"\"\" helper function for data pipeline - does all the things we need \"\"\"\n",
    "    # TODO: if only train, remove this partial part and just pass process_in_and_label\n",
    "    map_func = partial(process_inp_and_label, training=training)\n",
    "    data = data.map(map_func)\n",
    "    # arbitrarily set buffer_size - feel free to use sth. else\n",
    "    data = data.shuffle(buffer_size=SHUFFLE_SIZE)\n",
    "    data = data.batch(BATCH_SIZE)\n",
    "    data = data.prefetch(PREFETCH_SIZE)\n",
    "    return data\n",
    "\n",
    "# TODO: if only train, remove True argument\n",
    "train_data = data_pipeline(train_data, True)\n",
    "# TODO: possibly needs to be changed\n",
    "test_data = data_pipeline(test_data, False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DLzOt6EFST5"
   },
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hqzt1QbVFT8u"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wmAH3JmtFVUE"
   },
   "source": [
    "class Encoder(Model):\n",
    "    # TODO: Implement init. If necessary, add additional arguments to init. \n",
    "    # Please use an output dimension of 10 for the final dense layer.\n",
    "    def __init__():\n",
    "        super(Encoder, self).init()\n",
    "        raise NotImplementedError(\"TODO: Implement Encoder\")\n",
    "\n",
    "    # TODO: Implement call. If necessary, add additional arguments to call\n",
    "    def call(x):\n",
    "        raise NotImplementedError(\"TODO: Define call\")     "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lIIYajNGWpp"
   },
   "source": [
    "#### Check Encoder"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZWv2ZyYkGaR4"
   },
   "source": [
    "test_inp = tf.random.uniform((BATCH_SIZE, 28, 28, 1), maxval=1)\n",
    "# TODO: @Sascha: Maybe you need to adjust the init arguments?\n",
    "encoder = Encoder()\n",
    "out = encoder(x)\n",
    "# Assert that shape of out is BATCH_SIZE, 10\n",
    "debug.assert_equal(out.shape, tf.zeros(BATCH_SIZE, 10).shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJHXYVylJXir"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mdOuIUpEJdhL"
   },
   "source": [
    "class Encoder(Model):\n",
    "    # TODO: Implement init. If necessary, add additional arguments to init. \n",
    "    def __init__():\n",
    "        super(Encoder, self).init()\n",
    "        raise NotImplementedError(\"TODO: Implement Encoder\")\n",
    "\n",
    "    # TODO: Implement call. If necessary, add additional arguments to call\n",
    "    def call(x):\n",
    "        raise NotImplementedError(\"TODO: Define call\")     "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHgLrPmRJa2X"
   },
   "source": [
    "#### Check Decoder"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rRZd982JJlPK"
   },
   "source": [
    "test_inp = tf.random.uniform((BATCH_SIZE, 10))\n",
    "# TODO: @Sarah: Maybe you need to adjust the init arguments?\n",
    "decoder = Decoder()\n",
    "out = decoder(x)\n",
    "# Assert that shape of out is BATCH_SIZE, 28, 28, 1\n",
    "debug.assert_equal(out.shape, tf.zeros(BATCH_SIZE, 28, 28, 1).shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8xtVpF1VMo1s"
   },
   "source": [
    "# Leave following things to Hermann: Training, plotting "
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}